{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Import required libraries\n",
    "# ==========================================================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# ==========================================================\n",
    "# Load Fashion MNIST dataset (CSV format)\n",
    "# ==========================================================\n",
    "df = pd.read_csv(\"/kaggle/input/fashion-mnist-train-csv/fashion-mnist_train.csv\")\n",
    "\n",
    "# ==========================================================\n",
    "# Split the dataset into training and testing subsets\n",
    "# using stratified splitting per class\n",
    "# ==========================================================\n",
    "grouped = df.groupby(\"label\")\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for label, group in grouped:\n",
    "    train_split, test_split = train_test_split(\n",
    "        group, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        shuffle=True,\n",
    "        stratify=None  # Manual stratification by class\n",
    "    )\n",
    "    train_list.append(train_split)\n",
    "    test_list.append(test_split)\n",
    "\n",
    "# Concatenate splits and shuffle\n",
    "train_df = pd.concat(train_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat(test_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ==========================================================\n",
    "# Display one example image per class\n",
    "# ==========================================================\n",
    "examples = train_df.groupby(\"label\").first().reset_index()\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    img = examples.loc[i].drop(\"label\").values.astype(np.uint8).reshape(28, 28)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"Label: {examples.loc[i, 'label']}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================================\n",
    "# Prepare training and test data\n",
    "# ==========================================================\n",
    "X = train_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y = train_df[\"label\"].values\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]  # One-hot encoding\n",
    "\n",
    "X_test = test_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y_test = test_df[\"label\"].values\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "# ==========================================================\n",
    "# Standardize the data (zero mean, unit variance)\n",
    "# ==========================================================\n",
    "np.random.seed(0)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "X_test = (X_test - mean) / std  # Use same mean/std as training\n",
    "\n",
    "print(X.shape, y.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# ==========================================================\n",
    "# Activation functions with optional gradient computation\n",
    "# ==========================================================\n",
    "def relu(x, grad):\n",
    "    if grad:\n",
    "        return (x > 0).astype(float)\n",
    "    else:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x, grad):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    if grad:\n",
    "        return s * (1 - s)\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def softmax(z, grad):\n",
    "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# ==========================================================\n",
    "# Define neural network architecture\n",
    "# ==========================================================\n",
    "arch = [300, 300, 10]\n",
    "activations = [relu, relu, softmax]\n",
    "\n",
    "# Initialize parameters\n",
    "W = []\n",
    "B = []\n",
    "\n",
    "alpha = 0.002          # Learning rate\n",
    "batch = 32             # Batch size\n",
    "best_test = 0          # Best test accuracy observed\n",
    "lmbda = 0.001          # L2 regularization strength\n",
    "\n",
    "# ==========================================================\n",
    "# Data augmentation function\n",
    "# Random flipping and rotation\n",
    "# ==========================================================\n",
    "def augmentation(img):\n",
    "    if np.random.rand() < 0.5:\n",
    "        img = np.fliplr(img)\n",
    "    if np.random.rand() < 0.5:\n",
    "        angle = np.random.uniform(-15, 15)\n",
    "        M = cv2.getRotationMatrix2D((14, 14), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, (28, 28), borderMode=cv2.BORDER_REFLECT)\n",
    "    return img\n",
    "\n",
    "# ==========================================================\n",
    "# Optional label smoothing function (not used here)\n",
    "# ==========================================================\n",
    "def label_smoothing(y_one_hot, epsilon=0.1):\n",
    "    K = y_one_hot.shape[1]\n",
    "    return (1 - epsilon) * y_one_hot + epsilon / K\n",
    "\n",
    "# ==========================================================\n",
    "# Initialize weights and biases with He initialization\n",
    "# ==========================================================\n",
    "for i in range(len(arch)):\n",
    "    if i == 0:\n",
    "        w = np.random.randn(X.shape[1], arch[i]) * np.sqrt(2. / X.shape[1])\n",
    "    else:\n",
    "        w = np.random.randn(arch[i-1], arch[i]) * np.sqrt(2. / arch[i-1])\n",
    "    b = np.zeros((1, arch[i]))\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "\n",
    "# ==========================================================\n",
    "# Training loop\n",
    "# ==========================================================\n",
    "whole_accuracy = []\n",
    "whole_accuracy_test = []\n",
    "whole_cost = []\n",
    "whole_cost_test = []\n",
    "\n",
    "for e in range(600):  # Number of epochs\n",
    "    all_accuracy = []\n",
    "    all_accuracy_test = []\n",
    "    all_cost = []\n",
    "    all_cost_test = []\n",
    "    \n",
    "    # Training batches\n",
    "    for i in range(int(np.ceil(len(X) / batch))):\n",
    "        X_batch = X[batch * i : batch * (i + 1)]\n",
    "        y_batch = y[batch * i : batch * (i + 1)]\n",
    "        \n",
    "        # Data augmentation\n",
    "        X_image = X_batch.reshape(-1, 28, 28)\n",
    "        X_aug = np.array([augmentation(img) for img in X_image])\n",
    "        X_aug_flat = X_aug.reshape(-1, 784)\n",
    "        \n",
    "        # Forward pass\n",
    "        A = X_aug_flat\n",
    "        all_A = []\n",
    "        all_Z = []\n",
    "        m_batch = X_aug_flat.shape[0]\n",
    "        \n",
    "        for i in range(len(W)):\n",
    "            Z = A @ W[i] + B[i]\n",
    "            A = activations[i](Z, grad=False)\n",
    "            all_A.append(A)\n",
    "            all_Z.append(Z)\n",
    "        \n",
    "        # Compute cost with L2 regularization\n",
    "        cost = (-1 / m_batch) * np.sum(y_batch * np.log(A + 1e-8))\n",
    "        cost += (lmbda / (2 * m_batch)) * sum([np.sum(w ** 2) for w in W])\n",
    "        all_cost.append(cost)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        y_pred = np.argmax(A, axis=1)\n",
    "        y_true = np.argmax(y_batch, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true) * 100\n",
    "        all_accuracy.append(accuracy)\n",
    "        \n",
    "        # Backward pass and parameter update\n",
    "        for i in range(len(W) - 1, -1, -1):\n",
    "            if i == len(W) - 1:\n",
    "                dz = all_A[i] - y_batch\n",
    "            else:\n",
    "                dz = (dz @ W[i + 1].T) * activations[i](all_Z[i], grad=True)\n",
    "\n",
    "            if i == 0:\n",
    "                dw = X_aug_flat.T @ dz\n",
    "            else:\n",
    "                dw = all_A[i - 1].T @ dz\n",
    "\n",
    "            # Gradient descent step with L2 regularization\n",
    "            W[i] -= (alpha / m_batch) * (dw + lmbda * W[i])\n",
    "            B[i] -= (alpha / m_batch) * np.sum(dz, axis=0, keepdims=True)\n",
    "    \n",
    "    # Validation batches\n",
    "    for i_batch in range(int(np.ceil(len(X_test) / batch))):\n",
    "        X_test_batch = X_test[batch * i_batch : batch * (i_batch + 1)]\n",
    "        y_test_batch = y_test[batch * i_batch : batch * (i_batch + 1)]\n",
    "        A = X_test_batch\n",
    "        m_test = X_test_batch.shape[0]\n",
    "        \n",
    "        for i in range(len(W)):\n",
    "            Z = A @ W[i] + B[i]\n",
    "            A = activations[i](Z, grad=False)\n",
    "        \n",
    "        cost_test = (-1 / m_test) * np.sum(y_test_batch * np.log(A + 1e-8))\n",
    "        cost_test += (lmbda / (2 * m_test)) * sum([np.sum(w ** 2) for w in W])\n",
    "        all_cost_test.append(cost_test)\n",
    "        \n",
    "        y_pred_test = np.argmax(A, axis=1)\n",
    "        y_true_test = np.argmax(y_test_batch, axis=1)\n",
    "        accuracy_test = np.mean(y_pred_test == y_true_test) * 100\n",
    "        all_accuracy_test.append(accuracy_test)\n",
    "\n",
    "    # Track epoch results\n",
    "    whole_accuracy.append(np.mean(all_accuracy))\n",
    "    whole_accuracy_test.append(np.mean(all_accuracy_test))\n",
    "    whole_cost.append(np.mean(all_cost))\n",
    "    whole_cost_test.append(np.mean(all_cost_test))\n",
    "\n",
    "    # Save best model weights\n",
    "    if best_test < np.mean(all_accuracy_test):\n",
    "        best_test = np.mean(all_accuracy_test)\n",
    "        for k in range(len(W)):\n",
    "            np.save(f\"/kaggle/working/W{k}.npy\", W[k])\n",
    "            np.save(f\"/kaggle/working/B{k}.npy\", B[k])\n",
    "    \n",
    "    print(f\"epochs: {e} | best_test: {best_test:.2f}% | accuracy_train: {np.mean(all_accuracy):.2f}% | accuracy_test: {np.mean(all_accuracy_test):.2f}% | cost_train: {np.mean(all_cost):.4f} | cost_test: {np.mean(all_cost_test):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
